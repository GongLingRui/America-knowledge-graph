这份由东南大学 KGCODE 实验室发布的报告详细探讨了**知识图谱（KG）如何赋能大语言模型（LLM）**，重点解决大模型的“幻觉”问题并增强其在特定任务（如命名实体识别、知识图谱补全）中的表现 。

以下是对该文档的详细总结，以及从中提炼的注意点和易错点：

### 一、 文档核心内容总结

#### 1. 大模型幻觉的识别与分类

文档指出，LLM 经常产生与事实不符的输出，即“幻觉” 。幻觉主要分为两类：

* 
**事实幻觉 (Factuality Hallucination)**：生成内容与真实世界事实不一致 。


* 
**忠诚幻觉 (Faithfulness Hallucination)**：包括指令不一致（偏离用户指令）、上下文不一致（与提供的信息矛盾）及逻辑不一致 。



#### 2. 知识图谱赋能的关键技术框架

报告介绍了多个利用知识图谱增强 LLM 的前沿框架：

* 
**幻觉检测与抑制 (FactCHD & Retrofitting)**：通过从 LLM 输出中提取声明（Claim），在知识图谱中检索对应的三元组（Triple）进行验证，并根据验证结果修正（Retrofit）回答 。


* 
**ConsistNER (命名实体识别增强)**：针对低资源场景，结合“本体一致性”和“上下文一致性”来检索更准确的示例（Demonstration），提升 NER 性能 。


* 
**InstructNER (多模态增强)**：利用 Stable Diffusion 生成与文本相关的图片作为补充上下文，解决文本上下文匮乏导致的歧义问题 。


* 
**ANGEL (数据增强)**：利用知识图谱的本体结构引导 LLM 进行高质量的 NER 数据增强和标注 。


* 
**MPIKGC (知识图谱补全)**：通过多角度（描述扩展、关系理解、结构提取）提示 LLM，提升知识图谱补全的准确性 。


* 
**KG-GPT (通用推理框架)**：采用“分而治之”策略，将复杂问题分割成子句，检索对应子图，再由 LLM 进行逻辑推理 。



#### 3. 知识图谱与 RAG 的结合

* 
**RAG (检索增强生成)**：通过检索外部知识库减少幻觉，但存在忽视复杂关系、信息冗余及缺乏全局信息的问题 。


* 
**GraphRAG**：利用图形结构捕捉文本间的复杂关联，弥补传统 RAG 的不足 。



---

### 二、 核心注意点 (Key Points)

在实施知识图谱增强 LLM 的过程中，需特别注意以下维度：

* 
**声明提取的准确性**：这是幻觉检测的第一步。如果不能准确从生成文本中提取出待验证的原子声明（Claim），后续的检索和验证将失去意义 。


* 
**本体与上下文的双重对齐**：在低资源 NER 任务中，仅靠语义相似度检索示例是不够的，必须同时考虑实体的类型分布（本体一致性）和句子的实际语义（上下文一致性） 。


* 
**提示词工程 (Prompt Engineering)**：如在 MPIKGC 框架中，需要通过思维链 (CoT) 使 LLM 分步骤理解复杂关系和提取结构信息 。


* 
**多模态一致性校验**：在使用生成图片辅助 NER 时，必须使用搜索引擎图片进行事实性筛选，确保生成的图片不会引入新的干扰 。



---

### 三、 常见易错点 (Common Pitfalls)

根据文档中的“错误分析”及实验反馈，以下是常见的易错方向：

* 
**实体提取粒度不当**：实体提取过粗或过细都可能导致检索到过多的噪声三元组，从而干扰 LLM 的判断 。


* 
**事实选择失效**：在面对复杂或多跳（Multi-hop）问题时，系统往往难以从海量检索结果中精准选出有效的三元组作为证据 。


* 
**过度遵循流畅性**：LLM 在解码时可能为了保证语言流畅而忽视了对检索到的事实上下文的遵循，导致“幻觉”再次发生 。


* 
**忽略长尾知识**：传统链接预测方法在处理链接关系较少的长尾实体时效果较差，若不结合 LLM 的描述扩展能力，补全准确率会很低 。


* 
**线性化损失**：在将图谱三元组转化为 LLM 可理解的文本（线性化）时，如果处理不好，可能会丢失图谱原有的结构化语义 。
